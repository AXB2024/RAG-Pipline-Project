{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AXB2024/RAG-Pipline-Project/blob/main/Project_5_Final_Task_Measure_your_RAG_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V80sez2-HRkL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate sentence-transformers faiss-cpu llama-cpp-python unstructured PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg4EpdBTJA8p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import time\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_cpp import Llama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W0iUYIqJMT6"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Mount / Create Document Folder\n",
        "os.makedirs(\"/content/documents\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3654GyHJPDu"
      },
      "outputs": [],
      "source": [
        "# STEP 2: Extract Text from PDFs\n",
        "def extract_text_from_pdfs(folder=\"/content/documents\"):\n",
        "    docs = {}\n",
        "    for fname in os.listdir(folder):\n",
        "        if fname.endswith(\".pdf\"):\n",
        "            with fitz.open(os.path.join(folder, fname)) as doc:\n",
        "                full_text = \"\"\n",
        "                for page in doc:\n",
        "                    full_text += page.get_text()\n",
        "                docs[fname] = full_text\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAopBv2bJaNs"
      },
      "outputs": [],
      "source": [
        "# STEP 3: RAG Components\n",
        "queries = {\n",
        "    \"appraisal.pdf\": \"What is the estimated home value?\",\n",
        "    \"sample_bank_statement.pdf\": \"How much was the last transaction?\",\n",
        "    \"payslip_sample_image.pdf\": \"What is the total net salary for this month?\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gPehIssJkwm"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Split documents into chunks, embed them, and return passages, doc mapping, and embeddings\n",
        "def embed_documents(docs, embedder):\n",
        "    passages = []\n",
        "    doc_map = []\n",
        "    for name, text in docs.items():\n",
        "        for i in range(0, len(text), 300):\n",
        "            chunk = text[i:i+300]\n",
        "            passages.append(chunk)\n",
        "            doc_map.append(name)\n",
        "    embeddings = embedder.encode(passages, convert_to_tensor=True).cpu().numpy()\n",
        "    return passages, doc_map, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yr5SQhcJlpy"
      },
      "outputs": [],
      "source": [
        "# STEP 5: Convert query to embedding, perform FAISS similarity search, and return the best matching passage\n",
        "import numpy as np\n",
        "\n",
        "def search(query, embedder, passages, embeddings):\n",
        "    query_vec = embedder.encode([query])[0]\n",
        "    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)\n",
        "\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    D, I = index.search(query_vec, 1)\n",
        "    return passages[I[0][0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LcZiHtoJnRE"
      },
      "outputs": [],
      "source": [
        "# STEP 7: Load the specified model‚Äîeither a Hugging Face transformers model or a llama cpp GGUF model\n",
        "def load_model(name, model_type):\n",
        "    if model_type == \"transformers\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "        return lambda prompt: pipe(prompt, max_new_tokens=128, do_sample=True)[0]['generated_text']\n",
        "    elif model_type == \"llama-cpp\":\n",
        "        return Llama(model_path=name, n_ctx=2048, n_threads=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edTFjWU5JpI1"
      },
      "outputs": [],
      "source": [
        "# STEP 6: Generate an answer from the model given the query and retrieved context\n",
        "def generate_answer(model, query, context, model_type):\n",
        "    prompt = f\"Answer this question based on the context:\\nContext: {context}\\nQuestion: {query}\"\n",
        "    if model_type == \"llama-cpp\":\n",
        "        return model(prompt)[\"choices\"][0][\"text\"].strip()\n",
        "    else:\n",
        "        return model(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU-h5JhpJrgL"
      },
      "outputs": [],
      "source": [
        "# STEP 7: Run RAG\n",
        "def run_rag(model_name, model_type, embedder_name=\"all-MiniLM-L6-v2\"):\n",
        "    print(f\"\\nüîç Running RAG with model: {model_name}\")\n",
        "    embedder = SentenceTransformer(embedder_name)\n",
        "    documents = extract_text_from_pdfs()\n",
        "    passages, doc_map, embeddings = embed_documents(documents, embedder)\n",
        "    model = load_model(model_name, model_type)\n",
        "\n",
        "    for doc, query in queries.items():\n",
        "        print(f\"\\nüìÑ Document: {doc}\")\n",
        "        print(f\"‚ùì Query: {query}\")\n",
        "        start = time.time()\n",
        "        relevant = search(query, embedder, passages, embeddings)\n",
        "        answer = generate_answer(model, query, relevant, model_type)\n",
        "        end = time.time()\n",
        "        print(f\"üìå Retrieved: {relevant[:80]}...\")\n",
        "        print(f\"üí¨ Answer: {answer.strip()}\")\n",
        "        print(f\"‚ö° Speed: {round(end - start, 2)}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running All Models ‚Äî Phi-2, TinyLlama, Mistral (GGUF)\n"
      ],
      "metadata": {
        "id": "asSpVXEqC-yQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FOGdpHQPx-c"
      },
      "outputs": [],
      "source": [
        "run_rag(\"microsoft/phi-2\", \"transformers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryro1NNezmkP"
      },
      "outputs": [],
      "source": [
        "run_rag(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"transformers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nAfAClB1daN"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -O {\"/content/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KisnAKe0RGx"
      },
      "outputs": [],
      "source": [
        "run_rag(\"/content/mistral-7b-instruct-v0.2.Q4_K_M.gguf\", \"llama-cpp\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}